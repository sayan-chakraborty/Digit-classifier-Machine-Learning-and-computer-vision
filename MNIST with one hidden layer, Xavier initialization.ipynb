{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 60000)\n",
      "(784, 60000)\n",
      "(10, 10000)\n",
      "(784, 10000)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from keras.datasets import mnist #Keras is used only to get MNIST data\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "Y_train = Y_train.reshape(60000, 1).T \n",
    "b = np.zeros((60000, 10))\n",
    "b[np.arange(60000), Y_train] = 1\n",
    "Y_train = b.T #To convert to one hot matrix\n",
    "print(Y_train.shape)\n",
    "X_train = X_train.reshape(60000, 28, 28, 1)/255 #Divided by 255 to normalize the input data\n",
    "X_train = X_train.reshape(X_train.shape[0], -1).T #We use -1 because numpy will itself figure out what -1 is by looking at length of tha array and the remaining dimensions\n",
    "print(X_train.shape)\n",
    "Y_test = Y_test.reshape(10000, 1).T\n",
    "b = np.zeros((10000, 10))\n",
    "b[np.arange(10000), Y_test] = 1\n",
    "Y_test = b.T\n",
    "print(Y_test.shape)\n",
    "X_test = X_test.reshape(10000, 28, 28, 1)/255\n",
    "X_test = X_test.reshape(X_test.shape[0], -1).T\n",
    "print(X_test.shape)\n",
    "def relu(z):\n",
    "    z = z * (z > 0)\n",
    "    return z\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "learning_rate = 0.5\n",
    "m = 60000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.431183254897984\n",
      "2.081856653186116\n",
      "1.8079073831802708\n",
      "1.554369846171491\n",
      "1.452931407663296\n",
      "1.6435370003552245\n",
      "1.439358756571861\n",
      "1.232758705391723\n",
      "1.1182762560358297\n",
      "1.0025371205755522\n",
      "0.9478992863426068\n",
      "0.9294933990099056\n",
      "0.8601180039591388\n",
      "0.7888738276615026\n",
      "0.72038781583363\n",
      "0.705226334991696\n",
      "0.7509462748177111\n",
      "0.7652289754841437\n",
      "0.7495754062679988\n",
      "0.6035603880334954\n",
      "0.55823084475353\n",
      "0.5324623581742497\n",
      "0.5258598922109039\n",
      "0.5264220543803962\n",
      "0.5512654987769547\n",
      "0.5576611625856367\n",
      "0.6115965610192934\n",
      "0.5310958123850394\n",
      "0.5259968210848547\n",
      "0.4692033151601557\n",
      "0.45483729088075187\n",
      "0.4336792118101247\n",
      "0.42463357170142707\n",
      "0.41385475228832375\n",
      "0.40802230481137575\n",
      "0.4010627294659209\n",
      "0.39740797473214234\n",
      "0.3925064289159344\n",
      "0.3913031971041848\n",
      "0.38840785305278097\n",
      "0.39140451426767653\n",
      "0.3919616018525479\n",
      "0.4033157933519234\n",
      "0.41060749055668777\n",
      "0.43582031235173263\n",
      "0.4496203313672665\n",
      "0.4776039093525723\n",
      "0.46430680507909505\n",
      "0.4520794363811148\n",
      "0.40765050814153725\n",
      "0.3827572937724074\n",
      "0.3622901146214314\n",
      "0.3516936359457791\n",
      "0.3448988332957357\n",
      "0.34041458543981534\n",
      "0.33703337648808684\n",
      "0.33424221847657415\n",
      "0.33180170970631856\n",
      "0.3295757794867609\n",
      "0.32750276113361293\n",
      "0.3255402724555231\n",
      "0.32366526390754274\n",
      "0.3218632339282356\n",
      "0.3201217851705139\n",
      "0.3184333929986652\n",
      "0.31679388402050995\n",
      "0.3151992641700586\n",
      "0.31364734475969\n",
      "0.31213418501031864\n",
      "0.31065543086324493\n",
      "0.30920749741746556\n",
      "0.3077896263602057\n",
      "0.30639967128579676\n",
      "0.3050373274877273\n",
      "0.3036998834673727\n",
      "0.3023867698507972\n",
      "0.3010977116140587\n",
      "0.29983135297898567\n",
      "0.2985865774383161\n",
      "0.2973629115532161\n",
      "0.2961594606873018\n",
      "0.2949757138503957\n",
      "0.2938113892158934\n",
      "0.2926646034829182\n",
      "0.2915349388529752\n",
      "0.29042125652554523\n",
      "0.28932377002082804\n",
      "0.28824207032394517\n",
      "0.2871748712321542\n",
      "0.286122348984839\n",
      "0.28508502711221945\n",
      "0.2840623325830586\n",
      "0.2830529705249538\n",
      "0.2820557919920696\n",
      "0.28107103128622074\n",
      "0.2800980538844236\n",
      "0.2791370134667086\n",
      "0.2781876201327788\n",
      "0.2772501252432511\n",
      "0.2763235367356685\n",
      "0.2754064277688063\n",
      "0.2745000055389644\n",
      "0.27360332924038766\n",
      "0.2727159073719962\n",
      "0.2718381291772228\n",
      "0.27096892120246124\n",
      "0.270110218690696\n",
      "0.26926067064552794\n",
      "0.26841991155911193\n",
      "0.26758880427107384\n",
      "0.2667670090845796\n",
      "0.26595372490605274\n",
      "0.2651481631870628\n",
      "0.264350882042716\n",
      "0.26356138042182436\n",
      "0.26277805965376944\n",
      "0.2620018606241424\n",
      "0.26123271038418855\n",
      "0.260470076687762\n",
      "0.25971466521039716\n",
      "0.2589657879018768\n",
      "0.2582226530290873\n",
      "0.2574855515548326\n",
      "0.2567544979527365\n",
      "0.25603082129900917\n",
      "0.2553133196037617\n",
      "0.2546025453295551\n",
      "0.25389773661493165\n",
      "0.2531982754904169\n",
      "0.2525042155900438\n",
      "0.2518156391333673\n",
      "0.2511329113294356\n",
      "0.2504559129531957\n",
      "0.2497839749743709\n",
      "0.24911758501118517\n",
      "0.24845610916545086\n",
      "0.24779946758338814\n",
      "0.24714809737575583\n",
      "0.246501440780479\n",
      "0.24585932820480788\n",
      "0.2452214284996714\n",
      "0.244587663448921\n",
      "0.24395863145557353\n",
      "0.2433347240256626\n",
      "0.24271550482892706\n",
      "0.24210045861914997\n",
      "0.2414893980018992\n",
      "0.24088324935510524\n",
      "0.2402812911088872\n",
      "0.23968346909430285\n",
      "0.2390896213182038\n",
      "0.23849971670390074\n",
      "0.2379141525814109\n",
      "0.23733286801262135\n",
      "0.2367557228394738\n",
      "0.2361823335897893\n",
      "0.23561250212042348\n",
      "0.23504637064191325\n",
      "0.2344837629025685\n",
      "0.23392466122972252\n",
      "0.2333687659711892\n",
      "0.23281617347677677\n",
      "0.23226677459018588\n",
      "0.2317204699501866\n",
      "0.23117751806058415\n",
      "0.23063817837872885\n",
      "0.23010252712831994\n",
      "0.22957063593616717\n",
      "0.2290422187925642\n",
      "0.22851653612474043\n",
      "0.2279951120799635\n",
      "0.22747737263619522\n",
      "0.22696267486372007\n",
      "0.22645033649369253\n",
      "0.22594075926211168\n",
      "0.22543465895644227\n",
      "0.22493159712648594\n",
      "0.22443149855698039\n",
      "0.2239343685352717\n",
      "0.22344015292899666\n",
      "0.2229483858030468\n",
      "0.2224587937444753\n",
      "0.22197178348244406\n",
      "0.22148738040805646\n",
      "0.22100562094038706\n",
      "0.22052648021186078\n",
      "0.2200499802193702\n",
      "0.21957567727248448\n",
      "0.2191036685055466\n",
      "0.21863396684062436\n",
      "0.21816668303950587\n",
      "0.21770184859801509\n",
      "0.21723951853607873\n",
      "0.216779829026563\n",
      "0.21632275307916873\n",
      "0.21586837702510236\n",
      "0.21541631468464081\n",
      "0.21496665991463293\n",
      "0.21451949734408876\n",
      "0.2140743018632696\n",
      "0.21363091733107462\n",
      "0.21318959327400303\n",
      "0.212750359932615\n",
      "0.21231294462234612\n",
      "0.21187722645104637\n",
      "0.21144341916487036\n",
      "0.21101186649649137\n",
      "0.21058249600829687\n",
      "0.2101552818853012\n",
      "0.20972995077853232\n",
      "0.20930712649463948\n",
      "0.20888612652817393\n",
      "0.20846688411959902\n",
      "0.20804961523896928\n",
      "0.20763393238261282\n",
      "0.20722019360921437\n",
      "0.2068082122898815\n",
      "0.2063982314937506\n",
      "0.20599029230128704\n",
      "0.20558408461941258\n",
      "0.2051798709896599\n",
      "0.20477793815266263\n",
      "0.2043779838929458\n",
      "0.20397977363171946\n",
      "0.20358328752266228\n",
      "0.2031883378433995\n",
      "0.20279465304833735\n",
      "0.20240244166489507\n",
      "0.2020120922181307\n",
      "0.20162327711012035\n",
      "0.20123570687948222\n",
      "0.20084973811287474\n",
      "0.20046500891465166\n",
      "0.20008188591956583\n",
      "0.19970069461442141\n",
      "0.19932123019429487\n",
      "0.19894367726033263\n",
      "0.1985676631266905\n",
      "0.19819329385797574\n",
      "0.19782042807593694\n",
      "0.19744880395257441\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-6be5f56b7670>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mA1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mZ1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mZ2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mA2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mZ2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m60000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-1a38538e19cb>\u001b[0m in \u001b[0;36msoftmax\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m60000\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "W1 = np.random.randn(64, 784) * np.sqrt(2 / 784) #0.01 is multiplied to make gradient descent faster\n",
    "#W's are initialized randomly in order to break symmetry\n",
    "b1 = np.zeros((64, 1))\n",
    "W2 = np.random.randn(10, 64) * np.sqrt(2 / 64)\n",
    "b2 = np.zeros((10, 1))\n",
    "for i in range(2000):\n",
    "    #Forward propagation\n",
    "    Z1 = np.matmul(W1, X_train) + b1\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = np.matmul(W2, A1) + b2\n",
    "    A2 = softmax(Z2)\n",
    "    cost = np.sum(np.multiply(np.log(A2), Y_train))/ (-60000)\n",
    "    print(cost)\n",
    "    #Backward propagation\n",
    "    dZ2 = A2 - Y_train\n",
    "    dW2 = (1./60000) * np.matmul(dZ2, A1.T)\n",
    "    db2 = (1./60000) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    dA1 = np.matmul(W2.T, dZ2)\n",
    "    dZ1 = dA1 * (1 * (Z1 > 0))\n",
    "    dW1 = (1./m) * np.matmul(dZ1, X_train.T)\n",
    "    db1 = (1./m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    b1 = b1 - learning_rate * db1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is 94.42 %.\n"
     ]
    }
   ],
   "source": [
    "#Finding out the accuracy\n",
    "Z1 = np.matmul(W1, X_test) + b1\n",
    "A1 = relu(Z1)\n",
    "Z2 = np.matmul(W2, A1) + b2\n",
    "A2 = np.exp(Z2) / np.sum(np.exp(Z2), axis=0)\n",
    "predictions = np.argmax(A2, axis=0) #np.argmax axis = 0 return the values of the maximum indices along the columns\n",
    "labels = np.argmax(Y_test, axis=0)\n",
    "p = np.zeros((10000,))\n",
    "for i in range(10000):\n",
    "    if predictions[i] == labels[i]:\n",
    "        p[i] = 1\n",
    "    else:\n",
    "        p[i] = 0\n",
    "count = 0\n",
    "for i in range(10000):\n",
    "    if p[i] == 1:\n",
    "        count = count + 1\n",
    "accuracy = count / 100\n",
    "print('The accuracy is ' + str(accuracy) + ' %.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
