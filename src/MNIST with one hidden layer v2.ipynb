{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 60000)\n",
      "(784, 60000)\n",
      "(10, 10000)\n",
      "(784, 10000)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from keras.datasets import mnist #Keras is used only to get MNIST data\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "Y_train = Y_train.reshape(60000, 1).T \n",
    "b = np.zeros((60000, 10))\n",
    "b[np.arange(60000), Y_train] = 1\n",
    "Y_train = b.T #To convert to one hot matrix\n",
    "print(Y_train.shape)\n",
    "X_train = X_train.reshape(60000, 28, 28, 1)/255 #Divided by 255 to normalize the input data\n",
    "X_train = X_train.reshape(X_train.shape[0], -1).T #We use -1 because numpy will itself figure out what -1 is by looking at length of tha array and the remaining dimensions\n",
    "print(X_train.shape)\n",
    "Y_test = Y_test.reshape(10000, 1).T\n",
    "b = np.zeros((10000, 10))\n",
    "b[np.arange(10000), Y_test] = 1\n",
    "Y_test = b.T\n",
    "print(Y_test.shape)\n",
    "X_test = X_test.reshape(10000, 28, 28, 1)/255\n",
    "X_test = X_test.reshape(X_test.shape[0], -1).T\n",
    "print(X_test.shape)\n",
    "def relu(z):\n",
    "    z = z * (z > 0)\n",
    "    return z\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "learning_rate = 0.5\n",
    "m = 60000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.30309911445427\n",
      "2.299922538232706\n",
      "2.296461554799763\n",
      "2.292050219455312\n",
      "2.2859559256057547\n",
      "2.277235918228814\n",
      "2.26459712111057\n",
      "2.2462913147765633\n",
      "2.2201470610972107\n",
      "2.1836076122587302\n",
      "2.1338957246939674\n",
      "2.068695160857836\n",
      "1.9867499277921954\n",
      "1.888414830516837\n",
      "1.7763122425711895\n",
      "1.6555650293374427\n",
      "1.5328837765640906\n",
      "1.4148696914160401\n",
      "1.3064240371800317\n",
      "1.2099693782301044\n",
      "1.1257633802309763\n",
      "1.0527736932550165\n",
      "0.989478055478683\n",
      "0.9343539107664717\n",
      "0.886065768488285\n",
      "0.8435163112577859\n",
      "0.8058135699247684\n",
      "0.7722578113235106\n",
      "0.7423522895755645\n",
      "0.7160730560706234\n",
      "0.6955209355334954\n",
      "0.69236480894558\n",
      "0.7740972269169833\n",
      "0.9173816655396709\n",
      "1.1586196702715525\n",
      "1.095395535947583\n",
      "1.097608239068788\n",
      "0.8503456423773739\n",
      "0.6962289027825819\n",
      "0.6156994965357091\n",
      "0.5829718227677637\n",
      "0.5632170924829679\n",
      "0.5482657357015097\n",
      "0.5367342467625361\n",
      "0.5282022899733001\n",
      "0.5233996731321265\n",
      "0.52431455234961\n",
      "0.534636140243931\n",
      "0.5614018987691891\n",
      "0.6055128041708464\n",
      "0.6583096363274785\n",
      "0.6697302507185505\n",
      "0.6292963936180188\n",
      "0.5681948965503526\n",
      "0.5218443603471077\n",
      "0.4893744211297162\n",
      "0.4703270851315006\n",
      "0.45639882087632305\n",
      "0.44713825909762445\n",
      "0.43944040618334124\n",
      "0.4334515591829847\n",
      "0.4280967740143986\n",
      "0.4235784028260579\n",
      "0.4193936484471157\n",
      "0.41576839753170736\n",
      "0.4123423038177747\n",
      "0.40944847145691615\n",
      "0.40665488096010305\n",
      "0.4045577948965559\n",
      "0.40241808724633626\n",
      "0.40144225739570316\n",
      "0.4000121909160833\n",
      "0.4008618825597326\n",
      "0.4000488560660171\n",
      "0.40388328966448445\n",
      "0.40286076132540827\n",
      "0.410874497546774\n",
      "0.4071149073061719\n",
      "0.4188656034368434\n",
      "0.4089655081630815\n",
      "0.42090181690916456\n",
      "0.40512941601882474\n",
      "0.4140271525103349\n",
      "0.3977004623643556\n",
      "0.4037051762844408\n",
      "0.3912746712954244\n",
      "0.39534200048279156\n",
      "0.3879640105658389\n",
      "0.39002551122493545\n",
      "0.386751903939431\n",
      "0.38608563317202116\n",
      "0.38527982570679187\n",
      "0.3814990942031231\n",
      "0.3813868331347435\n",
      "0.3751585256535164\n",
      "0.37458433220260867\n",
      "0.36754869254961037\n",
      "0.3662808485832672\n",
      "0.35991413358862934\n",
      "0.35824440219916653\n",
      "0.35319274685446295\n",
      "0.3514565773653044\n",
      "0.3476297569821926\n",
      "0.34597825534990967\n",
      "0.3430736471234931\n",
      "0.3415581352104477\n",
      "0.3392835375260603\n",
      "0.33789768276363064\n",
      "0.33603597767223814\n",
      "0.3347551547293184\n",
      "0.33316461610968096\n",
      "0.3319722972179581\n",
      "0.33056757788179714\n",
      "0.32944850899892053\n",
      "0.3281774807548929\n",
      "0.3271168763335691\n",
      "0.3259414564525449\n",
      "0.32492927206241\n",
      "0.32383164597955705\n",
      "0.3228609523402295\n",
      "0.32182430065796497\n",
      "0.3208866694581867\n",
      "0.31989952968024865\n",
      "0.3189942853445171\n",
      "0.318045791497702\n",
      "0.31716839846003875\n",
      "0.31625270551557655\n",
      "0.3154003551325453\n",
      "0.3145145329835186\n",
      "0.3136849224091322\n",
      "0.3128270198512835\n",
      "0.31201573612423966\n",
      "0.3111797937675727\n",
      "0.31038706258566096\n",
      "0.30957313499188727\n",
      "0.3087961965946649\n",
      "0.3080008191121128\n",
      "0.307240961617102\n",
      "0.30646622863224315\n",
      "0.30572160581938773\n",
      "0.3049630270306643\n",
      "0.30423311491334537\n",
      "0.303491269957225\n",
      "0.30277610811786604\n",
      "0.3020478933695877\n",
      "0.30134554100209654\n",
      "0.30063327660128536\n",
      "0.2999436153144549\n",
      "0.29924533344771936\n",
      "0.29856634737125365\n",
      "0.2978803648330781\n",
      "0.2972120003000015\n",
      "0.29653580226940573\n",
      "0.29587653581745754\n",
      "0.2952091244427296\n",
      "0.2945586814767322\n",
      "0.29390050588521877\n",
      "0.29325806641851215\n",
      "0.2926075304332444\n",
      "0.2919711306905524\n",
      "0.2913284435541315\n",
      "0.2906988571189147\n",
      "0.2900632140097625\n",
      "0.2894414524059617\n",
      "0.28881432533870866\n",
      "0.28820160887788615\n",
      "0.28758201451687704\n",
      "0.2869752876896539\n",
      "0.286362479385348\n",
      "0.2857624150634387\n",
      "0.2851578915113749\n",
      "0.28456348330832504\n",
      "0.2839661670452828\n",
      "0.28337638011750377\n",
      "0.2827826841791556\n",
      "0.28219720176703905\n",
      "0.2816083659106283\n",
      "0.28102745664844775\n",
      "0.28044472407710314\n",
      "0.2798692181821323\n",
      "0.2792922826781616\n",
      "0.2787214970428044\n",
      "0.2781498466514687\n",
      "0.27758306061487104\n",
      "0.27701608467179384\n",
      "0.2764549657509008\n",
      "0.27589275901192584\n",
      "0.27533675129988816\n",
      "0.2747796863384762\n",
      "0.2742284922945191\n",
      "0.27367707746167363\n",
      "0.2731300994100954\n",
      "0.2725830266008806\n",
      "0.2720407617666932\n",
      "0.2714984243847066\n",
      "0.2709597168379686\n",
      "0.2704202196252264\n",
      "0.26988599837438787\n",
      "0.26935126551093874\n",
      "0.26881955514188527\n",
      "0.26828757413684984\n",
      "0.26775909406420956\n",
      "0.2672316795462538\n",
      "0.2667066527072373\n",
      "0.2661837885012913\n",
      "0.26566312358836636\n",
      "0.26514463813445527\n",
      "0.2646293725300847\n",
      "0.26411539637552206\n",
      "0.26360312335716424\n",
      "0.26309332041959627\n",
      "0.2625839458537942\n",
      "0.2620769737504715\n",
      "0.26157302946584454\n",
      "0.26107171464383244\n",
      "0.2605727036855667\n",
      "0.26007547331748\n",
      "0.25958064925987717\n",
      "0.25908898416378945\n",
      "0.2585989305022034\n",
      "0.25811139122660737\n",
      "0.25762595493982066\n",
      "0.25714268300879994\n",
      "0.2566610706589363\n",
      "0.25618207201887155\n",
      "0.2557048735482734\n",
      "0.2552294761913624\n",
      "0.2547553512508238\n",
      "0.25428326485060565\n",
      "0.2538132614584166\n",
      "0.2533456852781141\n",
      "0.25287989980193787\n",
      "0.25241562628972725\n",
      "0.2519525658122519\n",
      "0.25149045186642716\n",
      "0.25102972121104467\n",
      "0.25057016898525775\n",
      "0.250112001062989\n",
      "0.24965496278886784\n",
      "0.24919975402357397\n",
      "0.24874742442464481\n",
      "0.24829689988099826\n",
      "0.24784806900913478\n",
      "0.2474007770959043\n",
      "0.24695461669955426\n",
      "0.24651051328490864\n",
      "0.24606971249945397\n",
      "0.24563057105778155\n",
      "0.24519281802205647\n",
      "0.24475626379848903\n",
      "0.24432207934727046\n",
      "0.24388925409106077\n",
      "0.24345858165685502\n",
      "0.24302972753503074\n",
      "0.2426024998370797\n",
      "0.24217652502869386\n",
      "0.24175255620627112\n",
      "0.2413305191872197\n",
      "0.24091029694281418\n",
      "0.24049211822799754\n",
      "0.2400758799456882\n",
      "0.2396612851738297\n",
      "0.23924763814224728\n",
      "0.23883567421541957\n",
      "0.23842559131192276\n",
      "0.23801703134695856\n",
      "0.2376101487903686\n",
      "0.2372042698549812\n",
      "0.23680029144864245\n",
      "0.2363973711709936\n",
      "0.23599583643763677\n",
      "0.23559550292084605\n",
      "0.2351963584883668\n",
      "0.2347988229374988\n",
      "0.23440269644353737\n",
      "0.23400757185607718\n",
      "0.23361428349178337\n",
      "0.23322252380291195\n",
      "0.232832161460164\n",
      "0.23244269577222962\n",
      "0.23205482047727266\n",
      "0.23166851891439535\n",
      "0.2312833620569527\n",
      "0.2308994386946796\n",
      "0.23051664982815168\n",
      "0.2301350217057174\n",
      "0.22975467034830704\n",
      "0.22937545601396248\n",
      "0.22899700937703493\n",
      "0.22861980682008284\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-a48f351f0a25>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mdA1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdZ2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mdZ1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdA1\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mZ1\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mdW1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1.\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdZ1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[0mdb1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1.\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdZ1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mW2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mW2\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdW2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "W1 = np.random.randn(64, 784) * 0.01 #0.01 is multiplied to make gradient descent faster\n",
    "#W's are initialized randomly in order to break symmetry\n",
    "b1 = np.zeros((64, 1))\n",
    "W2 = np.random.randn(10, 64) * 0.01\n",
    "b2 = np.zeros((10, 1))\n",
    "for i in range(2000):\n",
    "    #Forward propagation\n",
    "    Z1 = np.matmul(W1, X_train) + b1\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = np.matmul(W2, A1) + b2\n",
    "    A2 = softmax(Z2)\n",
    "    cost = np.sum(np.multiply(np.log(A2), Y_train))/ (-60000)\n",
    "    print(cost)\n",
    "    #Backward propagation\n",
    "    dZ2 = A2 - Y_train\n",
    "    dW2 = (1./60000) * np.matmul(dZ2, A1.T)\n",
    "    db2 = (1./60000) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    dA1 = np.matmul(W2.T, dZ2)\n",
    "    dZ1 = dA1 * (1 * (Z1 > 0))\n",
    "    dW1 = (1./m) * np.matmul(dZ1, X_train.T)\n",
    "    db1 = (1./m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    b1 = b1 - learning_rate * db1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is 93.66 %.\n"
     ]
    }
   ],
   "source": [
    "#Finding out the accuracy\n",
    "Z1 = np.matmul(W1, X_test) + b1\n",
    "A1 = relu(Z1)\n",
    "Z2 = np.matmul(W2, A1) + b2\n",
    "A2 = np.exp(Z2) / np.sum(np.exp(Z2), axis=0)\n",
    "predictions = np.argmax(A2, axis=0) #np.argmax axis = 0 return the values of the maximum indices along the columns\n",
    "labels = np.argmax(Y_test, axis=0)\n",
    "p = np.zeros((10000,))\n",
    "for i in range(10000):\n",
    "    if predictions[i] == labels[i]:\n",
    "        p[i] = 1\n",
    "    else:\n",
    "        p[i] = 0\n",
    "count = 0\n",
    "for i in range(10000):\n",
    "    if p[i] == 1:\n",
    "        count = count + 1\n",
    "accuracy = count / 100\n",
    "print('The accuracy is ' + str(accuracy) + ' %.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
